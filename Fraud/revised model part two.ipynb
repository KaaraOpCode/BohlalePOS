{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8c7414-2315-4e33-b0f5-0a48f4cecdc5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988a35f-9244-4ede-b614-b588dc237da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 0Ô∏è‚É£ IMPORT LIBRARIES\n",
    "# ========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# For imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SHAP for explainability\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f7dba-5c1d-48d1-8421-8aae22a90127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 1Ô∏è‚É£ LOAD DATA\n",
    "# ========================================================\n",
    "df = pd.read_csv(r\"F:\\AI\\Project\\enhanced_fraud_dataset.csv\")\n",
    "df.head()\n",
    "df.info()\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d88cf-1ccc-484c-a3cd-3971d8c31463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 2Ô∏è‚É£ FEATURE ENGINEERING\n",
    "# ========================================================\n",
    "model_df = df.copy()\n",
    "\n",
    "model_df['has_anomaly'] = (model_df['anomalyFlags'] != 'NORMAL').astype(int)\n",
    "model_df['anomaly_count'] = model_df['anomalyFlags'].apply(lambda x: len(x.split('|')) if x != 'NORMAL' else 0)\n",
    "\n",
    "model_df['has_geo_anomaly'] = model_df['anomalyFlags'].str.contains('GEO').fillna(0).astype(int)\n",
    "model_df['has_amount_anomaly'] = model_df['anomalyFlags'].str.contains('AMOUNT').fillna(0).astype(int)\n",
    "model_df['has_velocity_anomaly'] = model_df['anomalyFlags'].str.contains('FREQ|SUCCESSION').fillna(0).astype(int)\n",
    "model_df['has_balance_anomaly'] = model_df['anomalyFlags'].str.contains('BALANCE').fillna(0).astype(int)\n",
    "\n",
    "categorical_columns = ['type', 'amount_category', 'risk_category']\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    model_df[f'{col}_encoded'] = le.fit_transform(model_df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "model_df['hour_sin'] = np.sin(2 * np.pi * model_df['hour_of_day']/24)\n",
    "model_df['hour_cos'] = np.cos(2 * np.pi * model_df['hour_of_day']/24)\n",
    "model_df['is_unusual_hours'] = ((model_df['hour_of_day'] < 6) | (model_df['hour_of_day'] > 22)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7429f-713b-4cef-91a9-13a6a75e4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 3Ô∏è‚É£ SELECT FEATURES & TARGET\n",
    "# ========================================================\n",
    "features = [\n",
    "    'amount', 'amount_to_balance_ratio', 'is_round_amount',\n",
    "    'hour_sin', 'hour_cos', 'is_weekend', 'is_unusual_hours',\n",
    "    'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest',\n",
    "    'distance_km', 'risk_score', 'anomaly_count', 'has_anomaly',\n",
    "    'has_geo_anomaly', 'has_amount_anomaly', 'has_velocity_anomaly', 'has_balance_anomaly',\n",
    "    'type_encoded', 'amount_category_encoded', 'risk_category_encoded'\n",
    "]\n",
    "\n",
    "features = [f for f in features if f in model_df.columns]\n",
    "X = model_df[features].fillna(0)\n",
    "y = model_df['isFraud']\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target distribution:\", y.value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d7b3b-0173-4249-b5b6-a18f8607af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 4Ô∏è‚É£ TRAIN-TEST SPLIT\n",
    "# ========================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set:\", X_train.shape, \"Test set:\", X_test.shape)\n",
    "print(\"Fraud ratio in train:\", y_train.mean()*100, \"%\")\n",
    "print(\"Fraud ratio in test:\", y_test.mean()*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424561c-d0c9-4277-af4a-21d5081651f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 6Ô∏è‚É£ SCALE FEATURES\n",
    "# ========================================================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502eaf8-c2bb-4c32-96b3-ad8ee0b247a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 7Ô∏è‚É£ MODEL TRAINING\n",
    "# ========================================================\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=12,\n",
    "    min_samples_split=15,\n",
    "    min_samples_leaf=8,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1921d0-b2ef-45b1-b79a-761af68a0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 8Ô∏è‚É£ MODEL EVALUATION\n",
    "# ========================================================\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "precision = (y_pred & y_test).sum() / (y_pred.sum() + 1e-8)\n",
    "recall = (y_pred & y_test).sum() / (y_test.sum() + 1e-8)\n",
    "f1 = 2 * (precision*recall) / (precision+recall+1e-8)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate','Fraud']))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicted Legit','Predicted Fraud'],\n",
    "            yticklabels=['Actual Legit','Actual Fraud'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74ace2-2216-48f5-aab8-13a722c9fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 9Ô∏è‚É£ FEATURE IMPORTANCE\n",
    "# ========================================================\n",
    "feature_imp = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(data=feature_imp.head(15), x='importance', y='feature', palette='viridis')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40941f-4d33-4ae3-a86b-124648725423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# üîü SAVE MODEL & ARTIFACTS\n",
    "# ========================================================\n",
    "model_artifacts = {\n",
    "    'model': model,\n",
    "    'features': features,\n",
    "    'scaler': scaler,\n",
    "    'feature_importance': feature_imp,\n",
    "}\n",
    "\n",
    "joblib.dump(model_artifacts, 'anomaly_detection_model.pkl')\n",
    "print(\"Model saved: 'anomaly_detection_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d09823-6378-47c8-9de2-81c03e330c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ PREDICTION FUNCTION\n",
    "# ========================================================\n",
    "def predict_anomaly_transaction(new_data, model_path='anomaly_detection_model.pkl', threshold=0.5):\n",
    "    artifacts = joblib.load(model_path)\n",
    "    model = artifacts['model']\n",
    "    features = artifacts['features']\n",
    "    scaler = artifacts['scaler']\n",
    "    \n",
    "    X_new = new_data[features].fillna(0)\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "    \n",
    "    probabilities = model.predict_proba(X_new_scaled)[:,1]\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'anomaly_probability': probabilities,\n",
    "        'is_anomaly_predicted': predictions,\n",
    "        'risk_level': pd.cut(probabilities, bins=[0,0.2,0.5,0.8,1.0],\n",
    "                             labels=['Low','Medium','High','Critical'])\n",
    "    })\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbde9da-9e4c-4a38-bfa3-c08ad176ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ TEST PREDICTION FUNCTION\n",
    "# ========================================================\n",
    "sample_data = X_test.iloc[:10].copy()\n",
    "predictions = predict_anomaly_transaction(sample_data)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bea68-035d-48a9-973e-ca884e6b2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ SHAP EXPLAINABILITY\n",
    "# ========================================================\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=features, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=features)\n",
    "\n",
    "sample_idx = 0\n",
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[1],\n",
    "    shap_values[1][sample_idx,:],\n",
    "    X_test.iloc[sample_idx,:],\n",
    "    feature_names=features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ae963-c224-4a89-a26b-de87d9d2d38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a38b5-36de-413a-9d84-26ff7973d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ AUTOMATIC HIGH-RISK TRANSACTION DETECTION\n",
    "# ========================================================\n",
    "def detect_high_risk_transactions(file_path, model_path='anomaly_detection_model.pkl', threshold=0.5):\n",
    "    new_data = pd.read_csv(file_path)\n",
    "    print(f\"Loaded {len(new_data)} transactions from {file_path}\")\n",
    "    \n",
    "    artifacts = joblib.load(model_path)\n",
    "    model = artifacts['model']\n",
    "    features = artifacts['features']\n",
    "    scaler = artifacts['scaler']\n",
    "    \n",
    "    for col in features:\n",
    "        if col not in new_data.columns:\n",
    "            new_data[col] = 0\n",
    "    \n",
    "    X_new = new_data[features].fillna(0)\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "    \n",
    "    probabilities = model.predict_proba(X_new_scaled)[:,1]\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "    \n",
    "    risk_levels = pd.cut(probabilities, bins=[0,0.2,0.5,0.8,1.0],\n",
    "                         labels=['Low','Medium','High','Critical'])\n",
    "    \n",
    "    results = new_data.copy()\n",
    "    results['anomaly_probability'] = probabilities\n",
    "    results['is_anomaly_predicted'] = predictions\n",
    "    results['risk_level'] = risk_levels\n",
    "    \n",
    "    high_risk = results[results['risk_level'].isin(['High','Critical'])].sort_values('anomaly_probability', ascending=False)\n",
    "    \n",
    "    print(f\"Detected {len(high_risk)} high/critical risk transactions\")\n",
    "    return high_risk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e719d9e-ad9a-460e-a7c4-14561bd7f5f3",
   "metadata": {},
   "source": [
    "TEST HIGH-RISK DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7452927-0310-4545-a684-919bd6f8cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"F:\\AI\\Project\\new_transactions.csv\"\n",
    "high_risk_transactions = detect_high_risk_transactions(file_path)\n",
    "high_risk_transactions.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955e547-25c9-489e-94be-dc1279f8a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT HIGH-RISK TRANSACTIONS TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb815a88-1101-4196-a5e7-6268bdbaeee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def export_high_risk_transactions(high_risk_df, output_dir=r\"F:\\AI\\Project\\Reports\"):\n",
    "    import os\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"high_risk_transactions_{timestamp}.csv\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    high_risk_df.to_csv(file_path, index=False)\n",
    "    print(f\"High-risk transactions exported to: {file_path}\")\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03367e6e-e84d-43c4-8d6b-90db3b5d2b71",
   "metadata": {},
   "source": [
    "TEST EXPORT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab9304-ee3d-4ad8-8104-10fcc1940635",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_file = export_high_risk_transactions(high_risk_transactions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
